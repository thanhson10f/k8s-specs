minikube docker-env
eval $(minikube docker-env)

minikube ssh

###Confirm that kubectl is also pointing to the Minikube VM
kubectl config current-context

kubectl get all --all-namespaces

minikube start --vm-driver=virtualbox
minikube stop
minikube delete

###Specifying the Kubernetes Version

minikube start \
    --vm-driver=virtualbox \
    --kubernetes-version="v1.14.0"
    

#Getting Started with Pods
With Kubernetes, the smallest unit is a Pod.
A Pod is a way to represent a running process in a cluster.

###Create pod
kubectl run db --image mongo
kubectl create -f pod/db.yml

###Delete pod
kubectl delete pod db
kubectl delete -f pod/db.yml

### Get more info about pods
kubectl get pods -o wide
kubectl describe pod db
kubectl describe -f pod/db.yml

### How pod scheduling works?
- API Server (master node)
- Scheduler (master node)
- Kubelet (run on each node)

1. Kubernetes client (kubectl) sent a request to the API server requesting creation of a Pod defined in the pod/db.yml file.

2. Since the scheduler is watching the API server for new events, it detected that there is an unassigned Pod.

3. The scheduler decided which node to assign the Pod to and sent that information to the API server.

4. Kubelet is also watching the API server. It detected that the Pod was assigned to the node it is running on.

5. Kubelet sent a request to Docker requesting the creation of the containers that form the Pod. In our case, the Pod defines a single container based on the mongo image.

6. Finally, Kubelet sent a request to the API server notifying it that the Pod was created successfully.

![alt process](5716507227062272.png "Process")


### Executing a new process
kubectl exec db ps aux
kubectl exec db --container db ps aux
kubectl exec -it db sh

### Getting Logs
kubectl logs db
Follow the log in realtime
kubectl logs -f db

## Anatomy of a Pod
- Pods are designed to run multiple cooperative processes that should act as a cohesive unit. Those processes are wrapped in containers.

- All the containers that form a Pod are running on the same machine. A Pod cannot be split across multiple nodes.

- All the processes (containers) inside a Pod share the same set of resources, and they can communicate with each other through localhost. One of those shared resources is storage.

- A volume (think of it as a directory with shareable data) defined in a Pod can be accessed by all the containers thus allowing them all to share the same data.


## Formatting the Ouput
kubectl get -f pod/go-demo-2.yml -o jsonpath="{.spec.containers[*].name}"


## Multi-container Pods
A frequent use case is multi-container Pods used for:

Continuous integration (CI)
Continious Delivery (CD)
Continuous Deployment processes (CDP)


## Liveness Probe
livenessProbe can be used to confirm whether a container should be running. If the probe fails, Kubernetes will kill the container and apply restart policy which defaults to Always.

    livenessProbe:
      httpGet:
        path: /this/path/does/not/exist
        port: 8080
      initialDelaySeconds: 5
      timeoutSeconds: 2 # Defaults to 1
      periodSeconds: 5 # Defaults to 10
      failureThreshold: 1 # Defaults to 3
      
We declared that the first execution of the probe should be delayed by five seconds (initialDelaySeconds), that requests should timeout after two seconds (timeoutSeconds), that the process should be repeated every five seconds (periodSeconds), and (failureThreshold) define how many attempts it must try before giving up 


Do not create Pods by themselves. Let one of the controllers create Pods for you.


# ReplicaSets (Controller)

## Understanding ReplicaSets
Most applications should be scalable and all must be fault tolerant. Pods do not provide those features, ReplicaSets do.
ReplicaSet’s primary function is to ensure that the specified number of replicas of a service are (almost) always running.

If you’re familiar with Replication Controllers, it is worth mentioning that ReplicaSet is the next-generation ReplicationController(deprecated). The only significant difference is that ReplicaSet has extended support for selectors.

kubectl get rs
kubectl get pods --show-labels

kubectl create -f rs/go-demo-2.yml

Sequential Breakdown of the Process #
The sequence of events that transpired with the kubectl create -f rs/go-demo-2.yml command is as follows.

Kubernetes client (kubectl) sent a request to the API server requesting the creation of a ReplicaSet defined in the rs/go-demo-2.yml file.

The controller is watching the API server for new events, and it detected that there is a new ReplicaSet object.

The controller creates two new pod definitions because we have configured replica value as 2 in rs/go-demo-2.yml file.

Since the scheduler is watching the API server for new events, it detected that there are two unassigned Pods.

The scheduler decided which node to assign the Pod and sent that information to the API server.

Kubelet is also watching the API server. It detected that the two Pods were assigned to the node it is running on.

Kubelet sent requests to Docker requesting the creation of the containers that form the Pod. In our case, the Pod defines two containers based on the mongo and api image. So in total four containers are created.

Finally, Kubelet sent a request to the API server notifying it that the Pods were created successfully.

### Deleting ReplicaSets
kubectl delete -f rs/go-demo-2.yml

Only delete replicasets without delete pods
kubectl delete -f rs/go-demo-2.yml --cascade=false

### Re-using the Same Pods 

kubectl create -f rs/go-demo-2.yml --save-config
kubectl apply -f rs/go-demo-2-scaled.yml

We could have created the ReplicaSet with apply in the first place, but we didn’t. The apply command automatically saves the configuration so that we can edit it later on. The create command does not do such thing by default so we had to save it with --save-config.

### Destroying a Pod
POD_NAME=$(kubectl get pods -o name \
   | tail -1)
kubectl delete $POD_NAME

### Removing a label 

POD_NAME=$(kubectl get pods -o name \
    | tail -1)
kubectl label $POD_NAME service-
kubectl describe $POD_NAME

Please note - at the end of the name of the label. It is the syntax that indicates that a label should be removed.


### Re-adding the Label
kubectl label $POD_NAME service=go-demo-2




# Services
Kubernetes Services provide addresses through which associated Pods can be accessed.
## Creating Services by Exposing Ports
###Exposing a Resource
kubectl expose rs go-demo-2 \
    --name=go-demo-2-svc \
    --target-port=28017 \
    --type=NodePort
    
Line 1: We specified that we want to expose a ReplicaSet (rs).

Line 2: The name of the new Service should be go-demo-2-svc.

Line 3: The port that should be exposed is 28017 (the port MongoDB interface is listening to).

Line 4: we specified that the type of the Service should be NodePort.

### Types of Services
There are other Service types we could have used to establish communication:
- NodePort
- ClusterIP: ClusterIP (the default type) exposes the port only inside the cluster. Such a port would not be accessible from anywhere outside. ClusterIP is useful when we want to enable communication between Pods and still prevent any external access.
If NodePort is used, ClusterIP will be created automatically.
- LoadBalancer: The LoadBalancer type is only useful when combined with cloud provider’s load balancer.
- ExternalName: ExternalName maps a service to an external address (e.g., kubernetes.io).

### Sequential Breakdown of the Process
The processes that were initiated with the creation of the Service are as follows:

Kubernetes client (kubectl) sent a request to the API server requesting the creation of the Service based on Pods created through the go-demo-2 ReplicaSet.

Endpoint controller is watching the API server for new service events. It detected that there is a new Service object.

Endpoint controller created endpoint objects with the same name as the Service, and it used Service selector to identify endpoints (in this case the IP and the port of go-demo-2 Pods).

kube-proxy is watching for service and endpoint objects. It detected that there is a new Service and a new endpoint object.

kube-proxy added iptables rules which capture traffic to the Service port and redirect it to endpoints. For each endpoint object, it adds iptables rule which selects a Pod.

The kube-dns add-on is watching for Service. It detected that there is a new service.

The kube-dns added db's record to the dns server (skydns).

kubectl describe svc go-demo-2-svc

Name:                    go-demo-2-svc
Namespace:               default
Labels:                  db=mongo
                         language=go
                         service=go-demo-2
                         type=backend
Annotations:             <none>
Selector:                service=go-demo-2,type=backend
Type:                    NodePort
IP:                      10.0.0.194
Port:                    <unset>  28017/TCP
TargetPort:              28017/TCP
NodePort:                 <unset>  31879/TCP
Endpoints:               172.17.0.4:28017,172.17.0.5:28017
Session Affinity:        None
External Traffic Policy: Cluster
Events:                  <none>


Line 1-2: We can see the name and the namespace. We did not yet explore namespaces (coming up later) and, since we didn’t specify any, it is set to default.

Line 3-6: Since the Service is associated with the Pods created through the ReplicaSet, it inherited all their labels. The selector matches the one from the ReplicaSet. The Service is not directly associated with the ReplicaSet (or any other controller) but with Pods through matching labels.

Line 9-13: Next is the NodePort type which exposes ports to all the nodes. Since NodePort automatically created ClusterIP type as well, all the Pods in the cluster can access the TargetPort. The Port is set to 28017. That is the port that the Pods can use to access the Service. Since we did not specify it explicitly when we executed the command, its value is the same as the value of the TargetPort, which is the port of the associated Pod that will receive all the requests. NodePort was generated automatically since we did not set it explicitly. It is the port which we can use to access the Service and, therefore, the Pods from outside the cluster. In most cases, it should be randomly generated, that way we avoid any clashes.


PORT=$(kubectl get svc go-demo-2-svc \
    -o jsonpath="{.spec.ports[0].nodePort}")
IP=$(minikube ip)
echo "http://$IP:$PORT"


kubectl delete svc go-demo-2-svc

apiVersion: v1
kind: Service
metadata:
  name: go-demo-2
spec:
  type: NodePort
  ports:
  - port: 28017
    nodePort: 30001
    protocol: TCP
  selector:
    type: backend
    service: go-demo-2
    
Line 1-4: You should be familiar with the meaning of apiVersion, kind, and metadata, so we’ll jump straight into the spec section.

Line 5: Since we already explored some of the options through the kubectl expose command, the spec should be relatively easy to grasp.

Line 6: The type of the Service is set to NodePort meaning that the ports will be available both within the cluster as well as from outside by sending requests to any of the nodes.

Line 7-10: The ports section specifies that the requests should be forwarded to the Pods on port 28017. The nodePort is new. Instead of letting the service expose a random port, we set it to the explicit value of 30001. Even though, in most cases, that is not a good practice, I thought it might be a good idea to demonstrate that option as well. The protocol is set to TCP. The only other alternative would be to use UDP. We could have skipped the protocol altogether since TCP is the default value but, sometimes, it is a good idea to leave things as a reminder of an option.

Line 11-13: The selector is used by the Service to know which Pods should receive requests. It works in the same way as ReplicaSet selectors. In this case, we defined that the service should forward requests to Pods with labels type set to backend and service set to go-demo. Those two labels are set in the Pods spec of the ReplicaSet.

kubectl create -f svc/go-demo-2-svc.yml
kubectl get -f svc/go-demo-2-svc.yml

Let’s take a look at the endpoint. It holds the list of Pods that should receive requests.

kubectl get ep go-demo-2 -o yaml


## ClusterIP

apiVersion: v1
kind: Service
metadata:
  name: go-demo-2-db
spec:
  ports:
  - port: 27017
  selector:
    type: db
    service: go-demo-2
    
    
This Service definition does not contain anything new.

- There is no type, so it’ll default to ClusterIP.
- Since there is no reason for anyone outside the cluster to communicate with the database, there’s no need to expose it using the NodePort type.
- We also skipped specifying the NodePort, since only internal communication within the cluster is allowed.
- The same is true for the protocol. TCP is all we need, and it happens to be the default one.
- Finally, the selector labels are the same as the labels that define the Pod.
    
### The readinessProbe
The readinessProbe should be used as an indication that the service is ready to serve requests. When combined with Services construct, only containers with the readinessProbe state set to Success will receive requests.

readinessProbe:
          httpGet:
            path: /demo/hello
            port: 8080
          periodSeconds: 1
        livenessProbe:
          httpGet:
            path: /demo/hello
            port: 8080

While livenessProbe is used to determine whether a Pod is alive or it should be replaced by a new one, the readinessProbe is used by the iptables. A Pod that does not pass the readinessProbe will be excluded and will not receive requests. In theory, requests might be still sent to a faulty Pod, between two iterations. Still, such requests will be small in number since the iptables will change as soon as the next probe responds with HTTP code less than 200, or equal or greater than 400.


### Defining Multiple Objects In The Same YAML file

Combine multiple YAML fiel to one with (---)

apiVersion: apps/v1
kind: ReplicaSet

---

apiVersion: v1
kind: Service

## Discovering Services process
Services can be discovered through two principal modes:

- Environment variables
- DNS

check environment variable

POD_NAME=$(kubectl get pod \
    --no-headers \
    -o=custom-columns=NAME:.metadata.name \
    -l type=api,service=go-demo-2 \
    | tail -1)

kubectl exec $POD_NAME env

### Sequential Breakdown of the Process

- Let’s go through the sequence of events related to service discovery and components involved.
- When the api container go-demo-2 tries to connect with the go-demo-2-db Service, it looks at the nameserver configured in /etc/resolv.conf. kubelet configured the nameserver with the kube-dns Service IP (10.96.0.10) during the Pod scheduling process.
- The container queries the DNS server listening to port 53. go-demo-2-db DNS gets resolved to the service IP 10.0.0.19. This DNS record was added by kube-dns during the service creation process.
- The container uses the service IP which forwards requests through the iptables rules. They were added by kube-proxy during Service and Endpoint creation process.
- Since we only have one replica of the go-demo-2-db Pod, iptables forwards requests to just one endpoint. If we had multiple replicas, iptables would act as a load balancer and forward requests randomly among Endpoints of the Service.

# Deployments

apiVersion: apps/v1
kind: Deployment
metadata:
  name: go-demo-2-db
spec:
  selector:
    matchLabels:
      type: db
      service: go-demo-2
  template:
    metadata:
      labels:
        type: db
        service: go-demo-2
        vendor: MongoLabs
    spec:
      containers:
      - name: db
        image: mongo:3.3
        ports:
        - containerPort: 28017
        
We will regularly add --record to the kubectl create commands. This allows us to track each change to our resources such as a Deployments.

kubectl create \
    -f deploy/go-demo-2-db.yml \
    --record
    
..
Events:
  Type   Reason            Age  From                  Message
  ----   ------            ---- ----                  -------
  Normal ScalingReplicaSet 2m   deployment-controller Scaled up replica set go-demo-2-db-75fbcbb5cd to 1
  
From the Events section, we can observe that the Deployment created a ReplicaSet. Or, to be more precise, that it scaled it. That is interesting.

It shows that Deployments control ReplicaSets. The Deployment created the ReplicaSet which, in turn, created Pods.

### Sequential Breakdown of the Process

- Kubernetes client (kubectl) sent a request to the API server requesting the creation of a Deployment defined in the deploy/go-demo-2-db.yml file.

- The deployment controller is watching the API server for new events, and it detected that there is a new Deployment object.

- The deployment controller creates a new ReplicaSet object.


### Updating Deployments
Updating the db Image
kubectl set image \
    -f deploy/go-demo-2-db.yml \
    db=mongo:3.4 \
    --record
    
### Defining a Zero-Downtime Deployment

apiVersion: apps/v1
kind: Deployment
metadata:
  name: go-demo-2-api
spec:
  replicas: 3
  selector:
    matchLabels:
      type: api
      service: go-demo-2
  minReadySeconds: 1
  progressDeadlineSeconds: 60
  revisionHistoryLimit: 5
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 1
...

- Line 5-7: The spec section has a few of the fields we haven’t seen before, and a few of those we are familiar with. The replicas and the selector are the same as what we used in the ReplicaSet from the previous chapter.

- Line 11: minReadySeconds defines the minimum number of seconds before Kubernetes starts considering the Pods healthy. We put the value of this field to 1 second. The default value is 0, meaning that the Pods will be considered available as soon as they are ready and, when specified, livenessProbe returns OK. If in doubt, omit this field and leave it to the default value of 0. We defined it mostly for demonstration purposes.

- Line 13: The next field is revisionHistoryLimit. It defines the number of old ReplicaSets we can rollback. Like most of the fields, it is set to the sensible default value of 10. We changed it to 5 and, as a result, we will be able to rollback to any of the previous five ReplicaSets.

- Line 14: The strategy can be either the RollingUpdate or the Recreate type. The latter will kill all the existing Pods before an update. Recreate resembles the processes we used in the past when the typical strategy for deploying a new release was first to stop the existing one and then put a new one in its place. This approach inevitably leads to downtime. The only case when this strategy is useful is when applications are not designed for two releases to coexist. Unfortunately, that is still more common than it should be. If you’re in doubt whether your application is like that, ask yourself the following question. Would there be an adverse effect if two different versions of my application are running in parallel? If that’s the case, a Recreate strategy might be a good choice and you must be aware that you cannot accomplish zero-downtime deployments.

### Deployment Strategies
Let’s look into a bit of detail of both Recreate and RollingUpdate strategies.

Recreate Strategy
The Recreate strategy is much better suited for our single-replica database. We should have set up the native database replication (not the same as Kubernetes ReplicaSet object), but, that is out of the scope of this chapter.

If we’re running the database as a single replica, we must have mounted a network drive volume. That would allow us to avoid data loss when updating it or in case of a failure. Since most databases (MongoDB included) cannot have multiple instances writing to the same data files, killing the old release before creating a new one is a good strategy when replication is absent. We’ll apply it later.

RollingUpdate Strategy
The RollingUpdate strategy is the default type, for a good reason. It allows us to deploy new releases without downtime. It creates a new ReplicaSet with zero replicas and, depending on other parameters, increases the replicas of the new one, and decreases those from the old one. The process is finished when the replicas of the new ReplicaSet entirely replace those from the old one.

When RollingUpdate is the strategy of choice, it can be fine-tuned with the maxSurge and maxUnavailable fields. The former defines the maximum number of Pods that can exceed the desired number (set using replicas). It can be set to an absolute number (e.g., 2) or a percentage (e.g., 35%). The total number of Pods will never exceed the desired number (set using replicas) and the maxSurge combined. The default value is 25%.

maxUnavailable defines the maximum number of Pods that are not operational. If, for example, the number of replicas is set to 15 and this field is set to 4, the minimum number of Pods that would run at any given moment would be 11. Just as the maxSurge field, this one also defaults to 25%. If this field is not specified, there will always be at least 75% of the desired Pods.

In most cases, the default values of the Deployment specific fields are a good option. We changed the default settings only as a way to demonstrate better all the options we can use. We’ll remove them from most of the Deployment definitions that follow.

### Checking rollout status
kubectl rollout status -w \
    -f deploy/go-demo-2-api.yml
    
kubectl rollout history \
    -f deploy/go-demo-2-api.yml

### Rolling Back or Rolling Forward?
Rolling back a release that introduced database changes is often not possible. Even when it is, rolling forward is usually a better option when practicing continuous deployment with high-frequency releases limited to a small scope of changes.

## Rolling Back with Kubernetes 

kubectl rollout undo \
    -f deploy/go-demo-2-api.yml
    
## Get objects label
kubectl get deployments \
    -l type=db,vendor=MongoLabs
    
## Update multiple objects
kubectl set image deployments \
    -l type=db,vendor=MongoLabs \
    db=mongo:3.4 --record
    
    
## Scaling the Deployment 
kubectl scale deployment \
    go-demo-2-api --replicas 8 --record
    
    
# Ingress
Ingress objects manage external access to the applications running inside a Kubernetes cluster.

Why Ingress Controllers are Required?
We need a mechanism that will accept requests on pre-defined ports (e.g., 80 and 443) and forward them to Kubernetes Services. It should be able to distinguish requests based on paths and domains as well as to be able to perform SSL offloading.

minikube addons list

minikube addons enable ingress

kubectl get pods -n kube-system \
    | grep ingress

apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: go-demo-2
  annotations:
    kubernetes.io/ingress.class: "nginx"
    ingress.kubernetes.io/ssl-redirect: "false"
    nginx.ingress.kubernetes.io/ssl-redirect: "false"
spec:
  rules:
  - http:
      paths:
      - path: /demo
        backend:
          serviceName: go-demo-2-api
          servicePort: 8080
          
Line 5: This time, metadata contains a field we haven’t used before. The annotations section allows us to provide additional information to the Ingress Controller. As you’ll see soon, Ingress API specification is concise and limited. That is done on purpose. The specification API defines only the fields that are mandatory for all Ingress Controllers. All the additional info an Ingress Controller needs is specified through annotations. That way, the community behind the Controllers can progress at great speed, while still providing basic general compatibility and standards.
Line 8: We specified the annotation nginx.ingress.kubernetes.io/ssl-redirect: "false" which tells the Controller that we do NOT want to redirect all HTTP requests to HTTPS. We’re forced to do so since we do not have SSL certificates for the exercises that follow.
Now that we shed some light on the metadata and annotations, we can move to the ingress specification.

Line 9-16: We specified a set of rules in the spec section. They are used to configure Ingress resource. For now, our rule is based on http with a single path and a backend. All the requests with the path starting with /demo will be forwarded to the service go-demo-2-api on the port 8080.

kubectl create \
    -f ingress/go-demo-2.yml \
    --record --save-config
    
    
## Sequential Breakdown of the Process

Let’s see, through a sequence diagram, what happened when we created the Ingress resource.

The Kubernetes client (kubectl) sent a request to the API server requesting the creation of the Ingress resource defined in the ingress/go-demo-2.yml file.

The ingress controller is watching the API server for new events. It detected that there is a new Ingress resource.

The ingress controller configured the load balancer. In this case, it is nginx which was enabled by minikube addons enable ingress command. It modified nginx.conf with the values of all go-demo-2-api endpoints.

curl -I -H "Host: acme.com" \
    "http://$IP"
    
# Volumes
Kubernetes Volumes solve the need to preserve the state across container crashes. In essence, Volumes are references to files and directories made accessible to containers that form a Pod. The significant difference between different types of Kubernetes Volumes is in the way these files and directories are created.

apiVersion: v1
kind: Pod
metadata:
  name: docker
spec:
  containers:
  - name: docker
    image: docker:17.11
    command: ["sleep"]
    args: ["100000"]
    volumeMounts:
    - mountPath: /var/run/docker.sock
      name: docker-socket
  volumes:
  - name: docker-socket
    hostPath:
      path: /var/run/docker.sock
      type: Socket
      
      
Part of the definition closely mimics the kubectl run command we executed earlier. The only significant difference is in the volumeMounts and volumes sections.

Line 9-10: We changed the command and the arguments to sleep 100000. That will give us more freedom since we’ll be able to create the Pod, enter inside its only container, and experiment with different commands.

Line 11: The volumeMounts field is relatively straightforward and is the same no matter which type of Volume we’re using. In this section, we’re specifying the mountPath and the name of the volume. The former is the path we expect to mount inside this container. You’ll notice that we are not specifying the type of the volume nor any other specifics inside the VolumeMounts section. Instead, we simply have a reference to a volume called docker-socket.

Line 14: The Volume configuration specific to each type is defined in the volumes section. In this case, we’re using the hostPath Volume type.

hostPath allows us to mount a file or a directory from a host to Pods and, through them, to containers. Before we discuss the usefulness of this type, we’ll have a short discussion about use-cases when this is not a good choice.

Do not use hostPath to store a state of an application. Since it mounts a file or a directory from a host into a Pod, it is not fault-tolerant. If the server fails, Kubernetes will schedule the Pod to a healthy node, and the state will be lost.

For our use case, hostPath works just fine. We’re not using it to preserve state, but to gain access to Docker server running on the same host as the Pod.

Line 15-18: The hostPath type has only two fields. The path represents the file or a directory we want to mount from the host. Since we want to mount a socket, we set the type accordingly. There are other types we could use.


Types of Mounts in hostPath: Directory, DirectoryOrCreate, File, FileOrCreate, Socket, CharDevice, BlockDevice

kubectl create \
  -f volume/docker.yml
  
cat volume/prometheus.yml | sed -e \
    "s/192.168.99.100/$(minikube ip)/g" \
    | kubectl create -f - \
    --record --save-config
    
minikube ssh sudo chmod +rw \
    /files/prometheus-conf.yml
    
    
With the exception of emptyDir, our choice of Volume type demonstrated in this chapter was not simply based on the ability to use them in a Minikube cluster. Each of these three Volume types will be an essential piece in the chapters that follow.

We’ll use hostPath to access Docker server from inside containers.

The gitRepo Volume type will be very significant once we start designing a continuous deployment pipeline.

The emptyDir type will be required as long as we’re using Minikube. Until we have a better solution for creating a Kubernetes cluster, emptyDir will continue to be used in our Minikube examples.

We have only scratched the surface with Volumes. There are at least two more that we should explore inside Minikube, and one when we change to a different solution for creating a cluster.

The Volumes that we’ll explore throughout the rest of the course are long enough subjects to deserve a separate chapter or, as we already mentioned, require that we get rid of Minikube.


# ConfigMap

kubectl create cm my-config \
    --from-literal=something=else \
    --from-literal=weather=sunny
    
kubectl create cm my-config \
    --from-env-file=cm/my-env-file.yml
    
# Secrets

kubectl get secrets

kubectl create secret \
    generic my-creds \
    --from-literal=username=jdoe \
    --from-literal=password=incognito

kubectl get secret my-creds \
    -o jsonpath="{.data.password}" \
    | base64 --decode
    
# Namespaces
Applications and corresponding objects often need to be separated from each other to avoid conflicts and other undesired effects.

kubectl get ns
kubectl --namespace kube-public get all

kubectl create ns testing
kubectl get ns

kubectl config set-context testing \
    --namespace testing \
    --cluster minikube \
    --user minikube
    
kubectl config view

kubectl config use-context testing

kubectl exec -it test -- curl \
    "http://go-demo-2-api.testing:8080/demo/hello"
    
So, the go-demo-2-api Service created a DNS based on that name. Actually, the full DNS entry is go-demo-2-api.svc.cluster.local. Both resolve to the same service go-demo-2-api which, in this case, runs in the default Namespace.

The third DNS entry we got is in the format <service-name>.<namespace-name>.svc.cluster.local. In our case, that is go-demo-2-api.default.svc.cluster.local. Or, if we prefer a shorter version, we could use go-demo-2-api.default.

In most cases, there is no good reason to use the <service-name>.<namespace-name> format when communicating with Services within the same Namespace.

The primary objective behind the existence of the DNSes with the Namespace name is when we want to reach services running in a different Namespace.

If we’d like to reach go-demo-2-api running in the testing Namespace from the test Pod in the default Namespace, we should use the go-demo-2-api.testing.svc.cluster.local DNS or, even better, the shorter version go-demo-2-api.testing.

# Securing Kuberneties Clusters

Checking out the Port
Typically, the Kubernetes API is served on a secured port. Our Minikube cluster is no exception. We can check the port from the kubectl config.

kubectl config view \
    -o jsonpath='{.clusters[?(@.name=="minikube")].cluster.server}'
    
kubectl config view \
    -o jsonpath='{.clusters[?(@.name=="minikube")].cluster.certificate-authority}'
    
    
Understanding the Process #
Each request to the API goes through three stages.

Authentication
Authorization
Passing the admission control
Authentication #
Kubernetes uses client certificates, bearer tokens, an authenticating proxy, or HTTP basic auth to authenticate API requests through authentication plugins. In the authentication process, the username is retrieved from the HTTP request. If the request cannot be authenticated, the operation is aborted with the status code 401.

Authorization #
Once the user is authenticated, the authorization validates whether it is allowed to execute the specified action. The authorization can be performed through ABAC, RBAC, or Webhook modes.

Passing the Admission Control #
Finally, once a request is authorized, it passes through admission controllers. They intercept requests to the API before the objects are persisted and can modify them. They are advanced topics that we won’t cover in this chapter.

Authentication is pretty standard, and there’s not much to say about it. On the other hand, admission controllers are too advanced to be covered just yet.


## Create Users to Access the Cluster

Creating a User #
The first thing we’ll do is to create a private key for John. We’ll assume that John Doe’s username is jdoe.
mkdir keys
openssl genrsa -out keys/jdoe.key 2048

Next, we’ll use the private key to generate a certificate.
openssl req -new \
    -key keys/jdoe.key \
    -out keys/jdoe.csr \
    -subj "/CN=jdoe/O=devs"
    
For the final certificate, we’ll need the cluster’s certificate authority (CA). It will be responsible for approving the request and for generating the necessary certificate John will use to access the cluster.

openssl x509 -req \
    -in keys/jdoe.csr \
    -CA ~/.minikube/ca.crt \
    -CAkey ~/.minikube/ca.key \
    -CAcreateserial \
    -out keys/jdoe.crt \
    -days 365
    
### Getting the Server Address 

SERVER=$(kubectl config view \
    -o jsonpath='{.clusters[?(@.name=="minikube")].cluster.server}')

echo $SERVER

Equipped with:

The new certificate (jdoe.crt)
The key (jdoe.key)
The cluster authority (ca.crt)
The address of the server,
John can configure his kubectl installation.

### Configuring kubectl

kubectl config set-cluster jdoe \
    --certificate-authority \
    keys/ca.crt \
    --server $SERVER
    
### Setting the Credentials

kubectl config set-credentials jdoe \
    --client-certificate keys/jdoe.crt \
    --client-key keys/jdoe.key

### Creating a New Context

kubectl config set-context jdoe \
    --cluster jdoe \
    --user jdoe

kubectl config use-context jdoe
kubectl config view

### The RBAC Components
Rules, Roles, Subjects, and RoleBindings.

### Rules

A Rule is a set of operations (verbs), resources, and API groups. Verbs describe activities that can be performed on resources which belong to different API Groups.
Permissions defined through Rules are additive. We cannot deny access to some resources.

If, for example, we’d like to allow a user only to create objects and retrieve their information, we’d use the verbs get, list and create. A verb can be an asterisk (*), thus allowing all verbs (operations).

Verbs are combined with Kubernetes resources. For example, if we’d like to allow a user only to create Pods and retrieve their information, we’d mix get, list and create verbs with the pods resource.

The last element of a Rule is the API Group. RBAC uses the rbac.authorization.k8s.io group. If we’d switch to a different authorization method, we’d need to change the group as well.

### Roles
A Role is a collection of Rules. It defines one or more Rules that can be bound to a user or a group of users.
The vital aspect of Roles is that they are applied to a Namespace. If we’d like to create a role that refers to a whole cluster, we’d use ClusterRole instead. Both are defined in the same way, and the only difference is in the scope (Namespace or an entire cluster).

### Subjects
Subjects define entities that are executing operations. A Subject can be a User, a Group, or a Service Account.


### RoleBindings

As the name suggests, RoleBindings bind Subjects to Roles.

Since Subjects define users, RoleBindings effectively bind users (or Groups or Service Accounts) to Roles, thus giving them permissions to perform certain operations on specific objects within a Namespace. Just like roles, RoleBindings have a cluster-wide alternative called ClusterRoleBindings. The only difference is that their scope is not limited to a Namespace, but applied to a whole cluster.


kubectl auth can-i get pods --as jdoe
kubectl auth can-i "*" "*"

kubectl get roles
kubectl get clusterroles
kubectl describe clusterrole view


### Creating Role Bindings
kubectl create rolebinding jdoe \
    --clusterrole view \
    --user jdoe \
    --namespace default \
    --save-config

kubectl get rolebindings
kubectl describe rolebinding jdoe

kubectl --namespace kube-system \
    describe rolebinding jdoe
    
kubectl auth can-i get pods \
    --as jdoe

kubectl auth can-i get pods \
    --as jdoe --all-namespaces
    
### Deleting the Role Binding
kubectl delete rolebinding jdoe

kubectl create -f auth/crb-view.yml \
    --record --save-config

apiVersion: v1
kind: Namespace
metadata:
  name: jdoe

---

apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: jdoe
  namespace: jdoe
subjects:
- kind: User
  name: jdoe
  apiGroup: rbac.authorization.k8s.io
roleRef:
  kind: ClusterRole
  name: cluster-admin
  apiGroup: rbac.authorization.k8s.io
  
kubectl create \
    -f auth/crb-release-manager.yml \
    --record --save-config
    
kubectl --namespace default auth \
    can-i create deployments --as jdoe
    
kubectl config set-context jdoe \
    --cluster jdoe \
    --user jdoe \
    --namespace jdoe

kubectl config use-context jdoe



# Managing Resources

minikube start --vm-driver=virtualbox
kubectl config current-context
minikube addons enable ingress
minikube addons enable metrics-server

## Definition
spec:
      containers:
      - name: db
        image: mongo:3.3
        resources:
          limits:
            memory: 200Mi
            cpu: 0.5
          requests:
            memory: 100Mi
            cpu: 0.3
            
Understanding CPU Resources #
CPU resources are measured in cpu units. The exact meaning of a cpu unit depends on where we host our cluster. If servers are virtualized, one cpu unit is equivalent to one virtualized processor (vCPU). When running on bare-metal with Hyperthreading, one cpu equals one Hyperthread. For the sake of simplification, we’ll assume that one cpu resource is one CPU processor (even though that is not entirely true).

If one container is set to use 2 CPU, and the other is set to 1 CPU, the later is guaranteed half as much processing power.

CPU values can be fractioned. In our example, the db container has the CPU requests set to 0.5 which is equivalent to half CPU. The same value could be expressed as 500m, which translates to five hundred millicpu. If you take another look at the CPU specs of the api container, you’ll see that its CPU limit is set to 200m and the requests to 100m. They are equivalent to 0.2 and 0.1 CPUs.

Limits and Requests #
We have already mentioned limits and requests quite a few times and yet we have not explained what each of them mean.

Limits #
A limit represents the amount of resources that a container should not pass. The assumption is that we define limits as upper boundaries which, when reached, indicate that something went wrong, as well as a way to guard our resources from being overtaken by a single rogue container due to memory leaks or similar problems.

If a container is restartable, Kubernetes will restart a container that exceeds its memory limit. Otherwise, it might terminate it. Bear in mind that a terminated container will be recreated if it belongs to a Pod (as all Kubernetes-controlled containers do).

Unlike memory, CPU limits never result in termination or restarts. Instead, a container will not be allowed to consume more than the CPU limit for an extended period.

Requests #
Requests represent the expected resource utilization. They are used by Kubernetes to decide where to place Pods depending on actual resource utilization of the nodes that form the cluster.

If a container exceeds its memory requests, the Pod it resides in might be evicted if a node runs out of memory. Such eviction usually results in the Pod being scheduled on a different node, as long as there is one with enough available memory.

If a Pod cannot be scheduled to any of the nodes due to lack of available resources, it enters the pending state waiting until resources on one of the nodes are freed, or a new node is added to the cluster.

### Metrics Server

kubectl top pods


Equipped with this knowledge, we can proceed to update our YAML definition. Still, before we do that, we need to clarify a few things.

The metrics we collected are based on applications that do nothing. Once they start getting a real load and start hosting production size data, the metrics would change drastically.

What you need is a way to predict how much resources an application will use in production, not in a simple test environment. You might be inclined to run stress tests that would simulate a production setup. It’s significant, but it does not necessarily result in real production-like behavior.

Replicating the production and behavior of real users is tough. Stress tests will get you half-way. For the other half, you’ll have to monitor your applications in production and, among other things, adjust resources accordingly.

There are many additional things you should take into account but, for now, we wanted to stress that applications that do nothing are not a good measure of resource usage. Still, we’re going to imagine that the applications we’re currently running are under production-like load and that the metrics we retrieved represent how the applications would behave in production.

ℹ️ Simple test environments do not reflect production usage of resources. Stress tests are a good start, but not a complete solution. Only production provides real metrics.
